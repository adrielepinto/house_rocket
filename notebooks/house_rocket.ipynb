{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d411a6",
   "metadata": {},
   "source": [
    "# 0.1 imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3abd315d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:27:47.448183Z",
     "start_time": "2022-06-08T14:27:47.437950Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime    import datetime\n",
    "import matplotlib.pyplot         as plt\n",
    "from IPython.core.display        import HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572359f",
   "metadata": {},
   "source": [
    "# 0.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e31c6f3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T17:33:23.281992Z",
     "start_time": "2022-06-07T17:33:23.274665Z"
    }
   },
   "outputs": [],
   "source": [
    "def jupyter_settings(): \n",
    "    %matplotlib inline \n",
    "    #%matplotlib notebook\n",
    "    %pylab inline\n",
    "    \n",
    "    plt.style.use('bmh')\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    \n",
    "     \n",
    "    display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option ('display.expand_frame_repr', False)\n",
    "    \n",
    "    sns.set()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a24d647b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T17:33:23.304023Z",
     "start_time": "2022-06-07T17:33:23.284683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "%matplotlib notebook\n",
    "\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams['figure.figsize'] = [25, 12]\n",
    "plt.rcParams['font.size'] = 24\n",
    "\n",
    "\n",
    "display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option ('display.expand_frame_repr', False)\n",
    "\n",
    "sns.set()\n",
    "\n",
    "jupyter_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a569ba5b",
   "metadata": {},
   "source": [
    "# 1.0 Loading Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e851b0cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-07T17:34:34.497509Z",
     "start_time": "2022-06-07T17:34:34.403791Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv ('/Users/adriele/Documents/repos/house_rocket/dataset/kc_house_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8ddf79",
   "metadata": {},
   "source": [
    "## 1.2 Data Dimersions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7be4e7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:19:50.891378Z",
     "start_time": "2022-06-08T14:19:50.887178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Cols 21613\n",
      "Number of Rows 21\n"
     ]
    }
   ],
   "source": [
    "print( 'Number of Cols {}'.format( df1.shape[0] ) ) \n",
    "print( 'Number of Rows {}'.format( df1.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35503a",
   "metadata": {},
   "source": [
    "## 1.3 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7175c9de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:21:09.967325Z",
     "start_time": "2022-06-08T14:21:09.953369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "date             0\n",
       "price            0\n",
       "bedrooms         0\n",
       "bathrooms        0\n",
       "sqft_living      0\n",
       "sqft_lot         0\n",
       "floors           0\n",
       "waterfront       0\n",
       "view             0\n",
       "condition        0\n",
       "grade            0\n",
       "sqft_above       0\n",
       "sqft_basement    0\n",
       "yr_built         0\n",
       "yr_renovated     0\n",
       "zipcode          0\n",
       "lat              0\n",
       "long             0\n",
       "sqft_living15    0\n",
       "sqft_lot15       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd877a",
   "metadata": {},
   "source": [
    "## 1.4 Check Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a38031b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:21:55.084416Z",
     "start_time": "2022-06-08T14:21:55.078444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 int64\n",
       "date              object\n",
       "price            float64\n",
       "bedrooms           int64\n",
       "bathrooms        float64\n",
       "sqft_living        int64\n",
       "sqft_lot           int64\n",
       "floors           float64\n",
       "waterfront         int64\n",
       "view               int64\n",
       "condition          int64\n",
       "grade              int64\n",
       "sqft_above         int64\n",
       "sqft_basement      int64\n",
       "yr_built           int64\n",
       "yr_renovated       int64\n",
       "zipcode            int64\n",
       "lat              float64\n",
       "long             float64\n",
       "sqft_living15      int64\n",
       "sqft_lot15         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9fc491",
   "metadata": {},
   "source": [
    "# 1.5 Changing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8f1d88a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:34:54.055490Z",
     "start_time": "2022-06-08T14:34:54.018190Z"
    }
   },
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44bfc6a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:34:54.820480Z",
     "start_time": "2022-06-08T14:34:54.804040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>2013-10-14</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>2025-02-15</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>2012-09-14</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id       date     price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  view  condition  grade  sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15\n",
       "0  7129300520 2013-10-14  221900.0         3       1.00         1180      5650     1.0           0     0          3      7        1180              0      1955             0    98178  47.5112 -122.257           1340        5650\n",
       "1  6414100192 2012-09-14  538000.0         3       2.25         2570      7242     2.0           0     0          3      7        2170            400      1951          1991    98125  47.7210 -122.319           1690        7639\n",
       "2  5631500400 2025-02-15  180000.0         2       1.00          770     10000     1.0           0     0          3      6         770              0      1933             0    98028  47.7379 -122.233           2720        8062\n",
       "3  2487200875 2012-09-14  604000.0         4       3.00         1960      5000     1.0           0     0          5      7        1050            910      1965             0    98136  47.5208 -122.393           1360        5000\n",
       "4  1954400510 2018-02-15  510000.0         3       2.00         1680      8080     1.0           0     0          3      8        1680              0      1987             0    98074  47.6168 -122.045           1800        7503"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9a9454c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:34:57.031457Z",
     "start_time": "2022-06-08T14:34:57.025990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        int64\n",
       "date             datetime64[ns]\n",
       "price                   float64\n",
       "bedrooms                  int64\n",
       "bathrooms               float64\n",
       "sqft_living               int64\n",
       "sqft_lot                  int64\n",
       "floors                  float64\n",
       "waterfront                int64\n",
       "view                      int64\n",
       "condition                 int64\n",
       "grade                     int64\n",
       "sqft_above                int64\n",
       "sqft_basement             int64\n",
       "yr_built                  int64\n",
       "yr_renovated              int64\n",
       "zipcode                   int64\n",
       "lat                     float64\n",
       "long                    float64\n",
       "sqft_living15             int64\n",
       "sqft_lot15                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d0608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "617f4fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T15:19:40.069259Z",
     "start_time": "2022-06-08T15:19:40.065542Z"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "loop = asyncio.new_event_loop()\n",
    "asyncio.set_event_loop(loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60e7384f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T15:19:42.213235Z",
     "start_time": "2022-06-08T15:19:42.209237Z"
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import geopandas\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from streamlit_folium import folium_static\n",
    "from folium.plugins import MarkerCluster\n",
    "from datetime import datetime\n",
    "import folium.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c82ec681",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T15:19:43.353134Z",
     "start_time": "2022-06-08T15:19:43.082802Z"
    }
   },
   "outputs": [
    {
     "ename": "InternalHashError",
     "evalue": "module '__main__' has no attribute '__file__'\n\nWhile caching the body of `get_data()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function get_data at 0x7fa5ac1bf0d0>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:368\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:634\u001b[0m, in \u001b[0;36m_CodeHasher._to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_should_be_hashed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_filename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    635\u001b[0m     context \u001b[38;5;241m=\u001b[39m _get_context(obj)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:410\u001b[0m, in \u001b[0;36m_CodeHasher._file_should_be_hashed\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_util\u001b[38;5;241m.\u001b[39mfile_is_in_folder_glob(\n\u001b[0;32m--> 410\u001b[0m     filepath, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_main_script_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m file_util\u001b[38;5;241m.\u001b[39mfile_in_pythonpath(filepath)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:721\u001b[0m, in \u001b[0;36m_CodeHasher._get_main_script_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;66;03m# This works because we set __main__.__file__ to the\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m# script path in ScriptRunner.\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m main_path \u001b[38;5;241m=\u001b[39m \u001b[43m__main__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__file__\u001b[39;49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(main_path))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalHashError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 244>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://data-seattlecitygis.opendata.arcgis.com/datasets/83fc2e72903343aabff6de8cb445b81c_2.geojson\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#url = '/Users/adriele/Documents/repos/python_zero_ao_ds/house_rocket/dataset/Zip_Codes.geojson'\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m geofile \u001b[38;5;241m=\u001b[39m get_geofile(url)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Transformation\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/caching.py:573\u001b[0m, in \u001b[0;36mcache.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m show_spinner:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m st\u001b[38;5;241m.\u001b[39mspinner(message):\n\u001b[0;32m--> 573\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_or_create_cached_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_or_create_cached_value()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/caching.py:498\u001b[0m, in \u001b[0;36mcache.<locals>.wrapped_func.<locals>.get_or_create_cached_value\u001b[0;34m()\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m cache_key\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# Delay generating the cache key until the first call.\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;66;03m# This way we can see values of globals, including functions\u001b[39;00m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# defined after this one.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;66;03m# If we generated the key earlier we would only hash those\u001b[39;00m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# globals by name, and miss changes in their code or value.\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m \u001b[43m_hash_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# First, get the cache that's attached to this function.\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# This cache's key is generated (above) from the function's code.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m mem_cache \u001b[38;5;241m=\u001b[39m _mem_caches\u001b[38;5;241m.\u001b[39mget_cache(cache_key, max_entries, ttl)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/caching.py:624\u001b[0m, in \u001b[0;36m_hash_func\u001b[0;34m(func, hash_funcs)\u001b[0m\n\u001b[1;32m    613\u001b[0m update_hash(\n\u001b[1;32m    614\u001b[0m     (func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m),\n\u001b[1;32m    615\u001b[0m     hasher\u001b[38;5;241m=\u001b[39mfunc_hasher,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m     hash_source\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    619\u001b[0m )\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Include the function's body in the hash. We *do* pass hash_funcs here,\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# because this step will be hashing any objects referenced in the function\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# body.\u001b[39;00m\n\u001b[0;32m--> 624\u001b[0m \u001b[43mupdate_hash\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhasher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_hasher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_funcs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhash_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_reason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHashReason\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCACHING_FUNC_BODY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhash_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m cache_key \u001b[38;5;241m=\u001b[39m func_hasher\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m    632\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmem_cache key for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m, cache_key\n\u001b[1;32m    634\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:116\u001b[0m, in \u001b[0;36mupdate_hash\u001b[0;34m(val, hasher, hash_reason, hash_source, context, hash_funcs)\u001b[0m\n\u001b[1;32m    113\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mhash_source \u001b[38;5;241m=\u001b[39m hash_source\n\u001b[1;32m    115\u001b[0m ch \u001b[38;5;241m=\u001b[39m _CodeHasher(hash_funcs)\n\u001b[0;32m--> 116\u001b[0m \u001b[43mch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:393\u001b[0m, in \u001b[0;36m_CodeHasher.update\u001b[0;34m(self, hasher, obj, context)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, hasher, obj: Any, context: Optional[Context] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;124;03m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(b)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:382\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InternalHashError(e, obj)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;66;03m# In case an UnhashableTypeError (or other) error is thrown, clean up the\u001b[39;00m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;66;03m# stack so we don't get false positives in future hashing calls\u001b[39;00m\n\u001b[1;32m    387\u001b[0m     hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:368\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    364\u001b[0m hash_stacks\u001b[38;5;241m.\u001b[39mcurrent\u001b[38;5;241m.\u001b[39mpush(obj)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# Hash the input\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tname, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;66;03m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mgetsizeof(b)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:634\u001b[0m, in \u001b[0;36m_CodeHasher._to_bytes\u001b[0;34m(self, obj, context)\u001b[0m\n\u001b[1;32m    632\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__code__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m code \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file_should_be_hashed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mco_filename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    635\u001b[0m     context \u001b[38;5;241m=\u001b[39m _get_context(obj)\n\u001b[1;32m    636\u001b[0m     defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__defaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:410\u001b[0m, in \u001b[0;36m_CodeHasher._file_should_be_hashed\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_is_blacklisted:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_util\u001b[38;5;241m.\u001b[39mfile_is_in_folder_glob(\n\u001b[0;32m--> 410\u001b[0m     filepath, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_main_script_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m file_util\u001b[38;5;241m.\u001b[39mfile_in_pythonpath(filepath)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/env_houserocket/lib/python3.9/site-packages/streamlit/legacy_caching/hashing.py:721\u001b[0m, in \u001b[0;36m_CodeHasher._get_main_script_directory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;66;03m# This works because we set __main__.__file__ to the\u001b[39;00m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m# script path in ScriptRunner.\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m main_path \u001b[38;5;241m=\u001b[39m \u001b[43m__main__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__file__\u001b[39;49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(main_path))\n",
      "\u001b[0;31mInternalHashError\u001b[0m: module '__main__' has no attribute '__file__'\n\nWhile caching the body of `get_data()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function get_data at 0x7fa5ac1bf0d0>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            "
     ]
    }
   ],
   "source": [
    "\n",
    "st.title ('House Rocket Company')\n",
    "st.markdown ( 'Welcome to Rouse Rocket Data Analysis')\n",
    "# st.set_page_config ( layuot = 'wide')\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def get_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    return data\n",
    "\n",
    "def get_geofile (url) :\n",
    "    geofile= geopandas.read_file( url )\n",
    "    return geofile\n",
    "\n",
    "def set_feature (data):\n",
    "    # add new features\n",
    "    data['price_m2'] = data['price'] / data['sqft_lot']\n",
    "    return data\n",
    "\n",
    "def overview_data (data):\n",
    "    # Data Overview\n",
    "    f_attributes = st.sidebar.multiselect('Enter columns', data.columns)\n",
    "    f_zipcode = st.sidebar.multiselect('Enter zipcode', data['zipcode'].unique())\n",
    "\n",
    "    st.title('Data Overview')\n",
    "\n",
    "    if (f_zipcode != []) & (f_attributes != []):\n",
    "        data = data.loc[data['zipcode'].isin(f_zipcode), f_attributes]\n",
    "\n",
    "    elif (f_zipcode != []) & (f_attributes == []):\n",
    "        data = data.loc[data['zipcode'].isin(f_zipcode):]\n",
    "\n",
    "    elif (f_zipcode == []) & (f_attributes != []):\n",
    "        data = data.loc[:, f_attributes]\n",
    "\n",
    "    else:\n",
    "        data = data.copy()\n",
    "    st.dataframe(data)\n",
    "\n",
    "    c1, c2 = st.columns((1, 1))\n",
    "\n",
    "    # Averange metrics\n",
    "    df1 = data[['id', 'zipcode']].groupby('zipcode').count().reset_index()\n",
    "    df2 = data[['price', 'zipcode']].groupby('zipcode').mean().reset_index()\n",
    "    df3 = data[['sqft_living', 'zipcode']].groupby('zipcode').mean().reset_index()\n",
    "    df4 = data[['price_m2', 'zipcode']].groupby('zipcode').mean().reset_index()\n",
    "\n",
    "    # Merge C1\n",
    "    m1 = pd.merge(df1, df2, on='zipcode', how='inner')\n",
    "    m2 = pd.merge(m1, df3, on='zipcode', how='inner')\n",
    "    df = pd.merge(m2, df4, on='zipcode', how='inner')\n",
    "\n",
    "\n",
    "    df.columns = ['ZIPCODE', 'TOTAL HOUSES', 'PRICE', 'SQFT LIVING', 'PRICE/2']\n",
    "    #st.write(df.head())\n",
    "    c1.header('Avarage Values')\n",
    "    c1.dataframe(df, height=400)\n",
    "\n",
    "    # startstic Descriptive C2\n",
    "    num_attributes = data.select_dtypes(include=['int64', 'float64'])\n",
    "    media = pd.DataFrame(num_attributes.apply(np.mean))\n",
    "    mediana = pd.DataFrame(num_attributes.apply(np.median))\n",
    "    std = pd.DataFrame(num_attributes.apply(np.std))\n",
    "\n",
    "    max_ = pd.DataFrame(num_attributes.apply(np.max))\n",
    "    min_ = pd.DataFrame(num_attributes.apply(np.min))\n",
    "\n",
    "    df1 = pd.concat([max_, min_, media, mediana, std], axis=1).reset_index()\n",
    "    df1.columns = ['attributes', 'max', 'min', 'mean', 'median', 'std']\n",
    "\n",
    "    c2.header('Descriptive Analysis')\n",
    "    c2.dataframe(df1, height=400)\n",
    "\n",
    "    return None\n",
    "\n",
    "def portfolio_density  (data, geofile):\n",
    "    # Densidade de portifolio\n",
    "    st.title('Region Overview')\n",
    "    c1, c2 = st.beta_columns((1, 1))\n",
    "    c1.header('Portfolio Density')\n",
    "\n",
    "    df = data.sample(10)\n",
    "    # Base map\n",
    "    density_map = folium.Map(location=[data['lat'].mean(),\n",
    "                             data['long'].mean()],\n",
    "                             default_zoom_start=15)\n",
    "\n",
    "    marker_cluster = MarkerCluster().add_to(density_map)\n",
    "    for name, row in df.iterrows():\n",
    "        folium.Marker([row['lat'], row['long']],\n",
    "                      popup='Sold R${0} on: {1}. Features: {2} sqft, {3} bedrooms, {4} bathrooms, year built {5}'.format(row['price'],\n",
    "                                                               row['date'],\n",
    "                                                               row['sqft_living'],\n",
    "                                                               row['bedrooms'],\n",
    "                                                               row['bathrooms'],\n",
    "                                                               row['yr_built'])).add_to(marker_cluster)\n",
    "\n",
    "    with c1:\n",
    "        folium_static(density_map)\n",
    "\n",
    "    # Region price\n",
    "    c2.header('Price Density')\n",
    "\n",
    "    df = data[['price', 'zipcode']].groupby('zipcode').mean().reset_index()\n",
    "    df.columns = ['ZIP', 'PRICE']\n",
    "\n",
    "    #df = df.sample(10)\n",
    "    geofile = geofile[geofile['ZIP'].isin(df['ZIP'].tolist())]\n",
    "\n",
    "    region_price_map = folium.Map(location=[data['lat'].mean(),\n",
    "                                            data['long'].mean()],\n",
    "                                  default_zoom_start=15)\n",
    "\n",
    "    region_price_map.choropleth(data=df,\n",
    "                                geo_data=geofile,\n",
    "                                columns=['ZIP', 'PRICE'],\n",
    "                                key_on='feature.properties.ZIP',\n",
    "                                fill_color='YlOrBr',\n",
    "                                fill_opacity=0.9,\n",
    "                                line_opacity=0.5,\n",
    "                                legend_name='AVG PRICE')\n",
    "\n",
    "\n",
    "    with c2:\n",
    "        folium_static(region_price_map)\n",
    "\n",
    "def commercial_distribution (data):\n",
    "\n",
    "    # Distribicao do imoveis por categoria\n",
    "\n",
    "    st.sidebar.title('Comercial Option')\n",
    "    st.title('Comercial Attribute')\n",
    "\n",
    "    # ---------- Avarange Price per Year\n",
    "\n",
    "    data['date'] = pd.to_datetime(data['date']).dt.strftime('%y-%m-%d')\n",
    "\n",
    "    # Filters\n",
    "    min_year_built = int(data['yr_built'].min())\n",
    "    max_year_built = int(data['yr_built'].max())\n",
    "\n",
    "    st.sidebar.subheader('Select Max Year Built')\n",
    "    f_year_built = st.sidebar.slider('Year Built',\n",
    "                                     min_year_built,\n",
    "                                     max_year_built,\n",
    "                                     min_year_built)\n",
    "    st.header('Avarange Price per Year Built')\n",
    "\n",
    "    # data select\n",
    "    df = data.loc[data['yr_built'] < f_year_built]\n",
    "    df = df[['yr_built', 'price']].groupby('yr_built').mean().reset_index()\n",
    "\n",
    "    # plot\n",
    "    fig = px.line(df, x='yr_built', y='price')\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # ---------- Avarange Price per day\n",
    "    st.header('Avarage Price per Day')\n",
    "    st.sidebar.subheader('Select Max Date')\n",
    "\n",
    "    # fielters\n",
    "    min_date = datetime.strptime(data['date'].min(), '%y-%m-%d')\n",
    "    max_date = datetime.strptime(data['date'].max(), '%y-%m-%d')\n",
    "\n",
    "    f_date = st.sidebar.slider('Date', min_date, max_date, min_date)\n",
    "\n",
    "    # data filtering\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    df = data.loc[data['date'] < f_date]\n",
    "    df = df[['date', 'price']].groupby('date').mean().reset_index()\n",
    "\n",
    "    # plot\n",
    "    fig = px.line(df, x='date', y='price')\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # ----------------- Histograma\n",
    "    st.header('Price Distribution')\n",
    "    st.subheader('Select Max Price')\n",
    "\n",
    "    # filter\n",
    "    min_price = int(data['price'].min())\n",
    "    max_price = int(data['price'].max())\n",
    "    avg_price = int(data['price'].mean())\n",
    "\n",
    "    # data filterin\n",
    "    f_price = st.sidebar.slider('Price', min_price, max_price, avg_price)\n",
    "    df = data.loc[data['price'] < f_price]\n",
    "\n",
    "    # data plot\n",
    "    fig = px.histogram( df, x='price', nbins=50 )\n",
    "    st.plotly_chart( fig, use_container_width=True )\n",
    "\n",
    "    return None\n",
    "\n",
    "def attributes_distribution (data):\n",
    "    # ================================================\n",
    "    # Distribuicao dos imoveis por categoria fisica\n",
    "    # ================================================\n",
    "    st.sidebar.title('Attributes Options')\n",
    "    st.title('House Attributes')\n",
    "\n",
    "    # Filters\n",
    "    f_bedrooms = st.sidebar.selectbox('Max number of bedrooms',\n",
    "                                      sorted(set(data['bedrooms'].unique())))\n",
    "    f_bathrooms = st.sidebar.selectbox('Max number of bedrooms',\n",
    "                                       sorted(set(data['bathrooms'].unique())))\n",
    "    c1, c2 = st.beta_columns(2)\n",
    "\n",
    "    # House per bedrooms\n",
    "    c1.header('Houses Bedroooms')\n",
    "    df = data[data['bedrooms'] < f_bedrooms]\n",
    "    fig = px.histogram(data, x='bedrooms', nbins=19)\n",
    "    c1.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # House per bathrooms\n",
    "    c2.header('Houses Bathrooms')\n",
    "    df = data[data['bathrooms'] < f_bathrooms]\n",
    "    px.histogram(df, x='bathrooms', nbins=50)\n",
    "    c2.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # Filters\n",
    "    f_floors = st.sidebar.selectbox('Max number of floor',\n",
    "                                    sorted(set (data['floors'].unique())))\n",
    "    f_waterview = st.sidebar.checkbox('Only Houses with Water View')\n",
    "    c1, c2 = st.beta_columns(2)\n",
    "\n",
    "    # House per Floors\n",
    "    c1.header('Houses per floor')\n",
    "    df = data [data['floors']> f_floors]\n",
    "    # Plot\n",
    "    px.histogram(df, x='floors', nbins=50)\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # House per water view\n",
    "    if f_waterview:\n",
    "        df = data[data['waterfront'] == 1]\n",
    "\n",
    "    else:\n",
    "        df = data.copy()\n",
    "\n",
    "    fig = px.histogram(df, x='waterfront', nbins=10)\n",
    "    c2.plotly_chart(fig, use_container_width=True)\n",
    "    return None\n",
    "\n",
    "if __name__== '__main__':\n",
    "    # ETL\n",
    "    # Data Extraction\n",
    "    path = '/Users/adriele/Documents/repos/python_zero_ao_ds/house_rocket/dataset/kc_house_data.csv'\n",
    "    url = 'http://data-seattlecitygis.opendata.arcgis.com/datasets/83fc2e72903343aabff6de8cb445b81c_2.geojson'\n",
    "    #url = '/Users/adriele/Documents/repos/python_zero_ao_ds/house_rocket/dataset/Zip_Codes.geojson'\n",
    "    data = get_data(path)\n",
    "    geofile = get_geofile(url)\n",
    "\n",
    "    # Transformation\n",
    "    data = set_feature (data)\n",
    "\n",
    "    overview_data (data)\n",
    "\n",
    "    portfolio_density (data, geofile)\n",
    "\n",
    "    commercial_distribution (data)\n",
    "\n",
    "    attributes_distribution (data)\n",
    "    # Loading\n",
    "\n",
    "\n",
    "# st.write (f_attributes)\n",
    "# st.write (data.head ())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f85495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d2060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
